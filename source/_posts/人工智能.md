---
title: 人工智能
date: 2024-04-25 00:00:00
categories: 人工智能
tag: 
	- CS课程笔记
---

<!-- toc -->

## 课程内容：
1. 人工智能绪论
2. 知识表示与推理
3. 搜索探寻与问题求解（含博弈）
4. 机器学习基础（含强化学习）
5. 神经网络与深度学习基础
6. 人工智能伦理与安全（自学）
7. 人工智能未来与展望（自学）
8. 课程总结

## 课程目标：

了解符号主义人工智能、连接主义人工智能和行为主义人工智能以及人工智能融合交叉等历史发展脉络，掌握知识表达与推理、搜索探寻与问题求解、统计机器学习、神经网络与深度学习、强化学习、人工智能博弈等基本算法，树立人工智能伦理与安全意识，理解保障人工智能安全、可信和公平的技术方法，完成相关应用案例实习。

# 第一章 人工智能绪论

## 一、人工智能起源与计算载体

**承载计算之能的器械如何产生、如何利用计算之器来模拟人类智能？**

- 什么是计算、计算的本质是什么？与该问题相关的可计算理论推动人类从手工计算迈入自动计算时代
- 如何模拟人类智能？符号逻辑(以推理为核心)、联结主义 (以统计机器学习为手段)和行为学派 (从环境交互过程中进行策略学习) 等不同技术手段模拟人类不同方面的智能

人工智能是以机器为载体所实现的人类智能或生物智能，因此人工智能也被称为机器智能(Machine Intelligence)。

原始递归函数和λ − 演算通过形式化方法进行“计算”，图灵机模型则通过机械化机制进行“计算”，因此图
灵机模型成为了现代计算机理论模型，推动了自动计算时代的到来，为人工智能领域提供了“计算之器”。

## 二、智能计算方法

### 人工智能的模型与方法

- 以符号注意为核心的逻辑推理
- 以问题求解为核心的探寻搜索
- 以数据驱动为核心的机器学习
- 以行为主义为核心的强化学习
- 以博弈对抗为核心的决策智能

## 三、人工智能知识点脉络

![Pasted image 20240227161005](../images/人工智能/Pasted%20image%2020240227161005.png)


## 本章小结

- 经典人工智能理论框架建立在以递归可枚举为核心的演绎逻辑和语义描述基础方法之上，由于先决条件问题(Qualification Problem，即枚举描述促发某一行为发生的所有前提条件)和隐性分支问题(RamificationProblem，即枚举刻画某个行为可能导致的所有后续潜在结果)的存在，使得我们难以事先拟好智能算法能够处理的所有情况，**因此智能算法在处理不确定性、开放性和动态性等问题时难以发挥作用**。
- 当前以数据建模和机器学习为核心的人工智能通过整合数据、模型和算力在计算机视觉、自然语言、语音识别等特定领域取得了显著进展，但是现有算法或系统还无法通过自身思考达到更高层次的智能，**它们与直觉推理、自适应和能力迁移等特点的通用人工智能 (Artificial GeneralIntelligence)依具有自我学习、然存在着差距**。
- 其作始也简，其将毕也必巨”，人工智能必将影响和推动人类社会的深刻变化。**“我们必须知道，我们必将知道 (Wir missen wissen, wir werden wissen)”**，相信大卫·希尔伯特David Hilbert)在1930年所持有的“可认知”乐观主义态度将鼓舞人工智能研究者和实践者进行不断探索。

# 第二章 智能体

## 一、总览AI的各种研究途径

- 符号主义
- 联结主义
- 行为主义

## 二、理性 Agent

智能体指任何能通过传感器感知环境和通过执行器作用于环境的实体。

理性依赖于四件事：
- 性能指标
- 先验知识
- 动作
- 感知序列

## 三、任务环境

PEAS是一种任务环境的规范：
- performance 性能
- enviroment 环境
- actuators 动作器
- sensors 感受器

不同的环境类型——任务环境的属性
- 完全可观测与部分可观测
- 单智能体与多智能体
- 确定性与随机性
- 阵发性与连续性
- 动态与静态
- 离散与连续
- 已知与未知

## 四、Agent的结构

一个智能体就是由一个由感知序列映射到动作的智能体函数来确定的。
f：P*->A
目标：找到实现理性智能体函数的方法。

智能体函数：
- 单个选项的效用计算
- 贯穿逻辑规则的推论
- 模糊逻辑
- 查找表

智能体的结构：
- agent = platform + agent program
- platfrom = computing device + sensors + actuators
- agent program ⊇ agent function

智能体的层次：
- 分层结构，包含许多“子智能体”
- 子智能体处理和执行较低级的任务
- 智能体和子智能体构建一个完整的系统，它可以通过行为和反应来完成艰巨的任务

表征智能体状态的三种方式：
- 原子式
- 因子式
- 结构式

## 五、智能体的主要类别

- 简单反射智能体
	- 外部环境完全可见的条件下才能工作
- 基于模型的反射智能体
	- 可处理部分可观测环境
	- 当前状态储存在智能体中，维护某种结构，它描述不可见外部环境的一部分
	- “关于外部环境如何运作”的知识被称为一个外部环境模型
	- 保持某种内部模型
	- 内部模型依赖于感知的历史，因此至少反射某些当前状态无法观测的方面
	- 作为反射智能体以某种方式选择动作
- 基于目标的智能体
	- 利用“目标”信息扩展了以上智能体功能
	- 目标信息描述所希望的情形
	- 允许在多个可能性中选择达成目标状态的那个
	- 搜索和规划致力于发现智能体目标的动作序列
	- 某些情况下不太有效
	- 更灵活，因为这种支持其决策的知识明显地展示出来，且可以被修改 
- 基于效用的智能体
	- 更通用的性能度量，允许对不同的外部环境状态进行比较
	- 效用描述智能体happiness
	- 将动作结果期待效应最大化
	- 需要建模并记录环境、任务的轨迹
	- 允许在位置环境中运行，且与最初知识相比会越来越胜任
- 学习智能体
	- 学习要素
	- 性能要素
	- 问题发生器

# 第三章 知识表示与推理

## 一、知识表示

- 规律性知识
	因果关系
- 事实性知识
	事物性质

- 确定性知识
- 不确定性知识

知识表示方法：
- 一阶谓词逻辑表示法
- 产生式表示法
	规则：IF P THEN Q（置信度）
	事实：（对象，属性，值，确信度）
- 框架表示法
	框架式一种描述所论对象属性的数据结构，一个框架由若干槽组成
	每一个槽可据情况划分为若干个侧面，每个侧面由若干个侧面值

## 二、知识推理

知识推理：
- 正向推理
- 逆向推理

非确定性推理：
- 规则运算
- 规则合成

产生式系统：
- 规则库
- 事实库
- 推理机

## 三、知识图谱

语义网络：
- 用关系图/多关系图来表示知识的结构化方式
- 顶点表示概念，边表示概念间的予以关系

语义网就是计算机能识别意义的互联网
结构化的数据式构建语义网的第一步

知识图谱：
- 知识图谱本质上是一种用语义网络来进行知识表示的知识库，图的顶点表示实体或概念，边表示关系
- 功能上看，知识图谱以结构化的形式描述客观世界中概念、实体间的复杂关系，将信息表达成接近人类认知世界的形式

知识图谱的架构：
1. 逻辑架构：模型层、数据层
2. 技术架构：
	- 信息抽取
	- 知识融合
	- 知识加工
	- 知识更新

知识图谱的应用：
- 智能搜索
- 反欺诈
- 不一致性验证
- 异常分析
- 静态分析
- 动态分析
- 失联客户管理

# 第四章 搜索技术——通过搜索进行问题求解

## 〇、引言：搜索技术概述

搜索技术就是用搜索方法寻求问题解答的技术。

在状态空间搜索最优的状态。

- 盲目搜索
- 启发式搜索
- 博弈搜索

## 一、问题求解Agent

### 人工智能中问题求解

- 问题求解智能体是一种基于目标的智能体，通过搜索解决问题。
- 对于某些NP完全或NP难问题，只能通过搜索解决。
- 问题形式化是给定一个目标，决定要考虑的动作与状态。
- 过程即寻找动作序列，称其为搜索。
- 解是一个达到目标的动作序列。

### 相关术语

- 状态空间
- 图
- 路径

### 形式化的五个要素

- 初始状态
- 动作
- 转换模型
- 目标测试
- 路径代价

## 二、问题实例

问题求解智能体三个基本步骤：
1. 形式化
	1. 目标形式化——成功的状态描述
	2. 问题形式化——根据所给的目标考虑行动和状态的描述
2. 搜索
	通过对行动序列代价计算来选取最佳的行动序列
3. 执行
	给出“解”执行行动

实例：
1. 真空吸尘器
2. 八数码问题
3. 八皇后问题

## 三、通过搜索求解

### 问题求解

搜素状态空间
- 通过显示生成树搜索
- 在一般的搜索生成树

基本思想：通过产生已经探索到的状态的后续状态的方法来离线地进行状态空间的模拟搜索

### 树搜索方法

基本思想：通过产生已经探索到的状态的后续状态的方法来离线地进行状态空间的模拟搜索

``` code
function TREE-SEARCH(problem,fringe) return a solution or failure
	fringeINSERT<-(MAKE-NODE(INITIAL-STATEIproblem]), fringe)
loop do
	if EMPTY?(fringe) then return failure
		nodeREMOVE-FIRST(fringe
if GOAL-TESTlproblem] applied to STATE[node] succeeds
	then return SOLUTION(node
fringe<-INSERT-ALL(EXPAND(node, problem), fringe)

function EXPAND(node, problem) return a set of nodes
	successors <- the empty set
		DEPTH[s]- DEPTHInode]+1
	for each <action, result> in SUCCESSOR-FNIproblem]STATE[node])
	do
		s<-a new NODE
		STATE[s] result
		PARENT-NODE[s]node
		ACTION[s] action
	PATH-COSTI[s] <- PATH-COSTInode] + STEP-COST(node, action,s)
	add s to successors
	return successors
```

### 图搜索方法

``` code
function GRAPH-SEARCH(problem) returns a solution, or failure
	initialize the fringe using the initial state of problem
	initialize the explored to be empty
	loop do
		if the fringe empty then return failure
		choose a leaf node and remove it from the fringe
		if the node contains a goal state then return the corresponding solution
		add the node to the explored
		expand the chosen node, adding the resulting nodes to the fringe
		only if not in the fringe or explored
```

## 四、无信息搜索策略

### 宽度优先搜索

- 优先扩展最浅层的未扩展节点
- 实现方法：
	Fringe表采用先进先出队列 ( FIFO queue）即新的后续节点总是放在队列的末尾
- 性能指标：
	完备性 YES （有限）
	时间复杂度 O(b^(d+1))
	空间复杂度 O(b^(d+1))
	最优性 YES（单步代价一致）

### 一致代价搜索

- 优先扩展具有最小代价的未扩展节点
- 实现: fringe 是根据路径代价排序的队列
- 在单步代价相等时与宽度优先搜索一样
- 性能指标：
	完备性? Yes,只要单步代价不是无穷小
	时间?代价小于最优解的节点个数，O(b^ceiling(C*/ ε))
	空间？代价小于最优解的节点个数，O(b^ceiling(C/ ε))
	最优性? Yes - 节点是根据代价排序扩展的

注：
1. C\*最优解的代价
2. ε是至少每个动作的代价
3. ceiling取上整

### 深度优先搜索

- 扩展最深层的未扩展节点
- 实现：fringe=后进先出队列（LIFO queue）
- 性能指标：
	完备性? No：在无限状态空间中不能保证找到解
	时间?O(b^m)
	空间？O(bm)，i.e.，线性空间！
	最优性? No

### 深度优先搜索改进

#### 有深度限制地深度优先搜索——深度有限搜索

- 性能指标：
	完备性? No
	时间?O(b^l)
	空间？O(bl)，i.e.，线性空间！
	最优性? No

#### 迭代深入搜索

- 性能指标：
	完备性? Yes
	时间?O(b^d)
	空间？O(bd)
	最优性? Yes，只要单步代价相等

### 双向搜索

- 从初始状态和目标状态同时出发
- 原理: b^d/2+b^d/2<b^d
- 检查当前节点是否是其他fringe表的节点
- 空间复杂度仍然是最大的问题
- 如果双向都采用宽度优先则算法是完备的和最优性的

### 无信息搜索算法性能评价小结


| 评价    | 宽度优先      | 代价一致                | 深度优先      | 深度有限         | 迭代深入     | 双向搜索          |
| ----- | --------- | ------------------- | --------- | ------------ | -------- | ------------- |
| 完备性   | YES       | YES                 | NO        | YES,if l >=d | YES      | YES           |
| 时间复杂度 | $b^{d+1}$ | $b^{C^*/ \epsilon}$ | $b^m$     | $b^l$        | $b^d$    | $b^{d/2}$     |
| 空间复杂度 | $b^{d+1}$ | $b^{C^*/ \epsilon}$ | $b^m$<br> | $bl$<br>     | $bd$<br> | $b^{d/2}$<br> |
| 最优性   | YES       | YES                 | NO        | NO           | YES      | YES           |

## 五、有信息搜索策略

### 最佳优先搜索

- 思想：使用一个评估函数 f(n)给每人节点估计他们的希望值。 优先扩展最有希望的未扩展节点。
- 实现：fringe表中节点根据评估值从“大到小”排序
- 最佳优先搜索策略有如下二种代表：
	- 贪婪最佳优先搜索
	- A搜索
- 评估函数：f(n)=h(n)=估计从节点n到目标的代价
- 贪婪最佳优先搜索优先扩展看上去更接近目标的节点 (启发式函数评估出来的)
- 性能指标：
	完备性? No，可能陷入死循环
	时间?O(b^m)
	空间？O(b^m)
	最优性? No

### A\*搜索

- 思想：避免扩展代价已经很高的节点
- 评估函数 f(n) = g(n) + h(n)
	- g(n)= 到达节点n已经发生的实际代价
	- h(n)= 从节点n到目标的代价估计值
	- f(n)=评估函数，估计从初始节点出发，经过节点n，到目标的路径代价的估计

可采纳的启发式函数：
- 如果启发式函数h(n)对于任意的节点n都满足 h(n)< h(n)，这里 h(n)是指从节点n到达目标的真正代价，则称h(n)是可采纳的 (admissible heuristic)
- 定理: 如果h(n)是可采纳的，则A 树搜索算法是具有最优性

- 性能指标：
	完备性? Yes
	时间?指数级
	空间？将所有产生的节点存储在内存中
	最优性? Yes

### A\*的改进方法

存储受限制的启发式搜索：
	思想：迭代加深的A\*算法，用一个f-cost代替有限制的深度优先中的d作为截断值，进行剪枝。

递归最佳优先搜索 (RBFS）
- 尝试模拟标准的最佳优先搜索而只需线性空间
	1. 记录当前节点的祖先可得到的最佳可替换路径的f值
	2. 如果当前的f值超过了这个限制，则递归将转回到替换路径
	3. 向上回溯改变f值到它的孩子的最佳f值
	4. 重复扩展这个上个节点，因为仍有可能存在较优解

存储限制的A*
- 当内存满了的时候删除最坏的节点

### 如何构建启发函数

- 可采纳的启发式可以源自简化版问题的一个精确解。称为松驰问题: 对原定问题的动作的约束放宽，以松驰化问题的最优解的代价来定义原问题的一个可采纳启发式函数。

- 可采纳的启发式可以从原问题的子问题的一个解得到这个代价是真实问题代价的下界。
- 将每个可能的子问题实例的精确解存储起来作为一个模式数据库。
- 用这个模式数据库的构建一个完整的启发式。

- 另一种方法就是从经验中学习

### 通过搜索进行问题求解一一有信息的搜索

- 最佳优先搜索 f(n)=g(n)+h(n)
- 贪婪最佳优先搜索 f(n)=h(n)
- A* 搜索：其中h(n)<=h*(n)是可采纳的
- A\*的改进
- 启发函数的启发能力
- 可采纳启发式设计：松驰问题、模式数据库、学习机制

# 第四章 搜索技术——复杂环境中的搜索

## 一、复杂环境中的搜索概述

经典搜索具有如下特点：
- 搜索算法被设成系统地探索问题的空间
- 该系统性由以下方法得到：在内存中保持一条或多条路径，并且在沿着该路径的每个点上记录哪些已被探索过
- 目标被找到时，该路径也就构成问题的一个解

然而，在许多问题中到达目标的路径是无关紧要的。

局部搜索具有以下特点：
- 它不介意什么路径
- 局部搜索算法使用一个当前节点（而不是多条路径），并且通常仅移动到该节点相邻的节点
- 通常，搜索后不保留该路径
- 优点：
	1. 使用很少内存
	2. 在大的或无限的状态空间中，能发现合理的解

局部搜索的优化问题：
- 在很多的最优化问题中，不需要知道到达目标的路径，目标状态本身才是问题的解
- 对于这类问题，可以采用局部搜索算法，通过不断改进当前状态，知道它满足约束为止

局部搜索的应用：
- 集成电路设计
- 工厂车间布局
- 车间作业调度
- 自动程序设计
- 通讯网络优化
- 车辆路由
- 投资组合管理

## 二、局部搜索

### 爬山法搜索

- 爬山搜索是一种属于局部搜索家族的数学优化方法。
- 爬山法基本思想：
	- 是一个沿着值增加的方向持续移动的简单循环过程
	- 当到达一个山峰时就停止
	- 爬山法不根据当前状态考虑计划未来后继节点。就像雾中登山一样
	- 当有多个继节点时，爬山法选取最优后继节点集中的一个
- 特点：大多数基本的局部搜索算法都不保持一棵搜索树。
- 爬山法在以下三种情况容易被困：
	1. 局部最大值
	2. 高原
	3. 山脊
- 爬山法变种：
	1. 随机爬山法
	2. 首选爬山法
	3. 随机重启开始爬山法
	4. 侧向移动

### 局部束搜索

- 在内存中仅保存一个节点似乎是对内存限制问题的极端反应。
- 局部束搜索：保存k状态，而不是一个状态
	- 从k个随机产生的状态开始
	- 每次迭代,产生k个状态的所有后续
	- 如果任何一个新产生的状态是目标状态，则停止。否则从所有后续中选择k个最好的后续，重复迭代
	- 同随机重新开始的区别：在k条线程中共享信息
	- 缺点：局部束搜索会很快地集中在状态空间的某个小区域内，使得搜索代价比爬山法还要昂贵。缺乏多样性。
- 改进的随机变种：它不是选择k个最佳后继节点，而是以选择后继节点的概率是其值的递增函数，来随机地选择k个后继节点。随机束搜索有些类似于自然选择的过程

### 禁忌搜索

算法思想：禁忌搜索 (Tabu Search，TS)模仿人类的记忆功能，在搜索过程中标记已经找到的局部最优解及求解过程并于之后的搜索中避开它们；算法通过禁忌策略实现记忆功能通过特赦(破禁)准则继承局部搜索
的强局部搜索能力。使得TS一方面具备高局部搜索能力，同时又能防止算法在优化中陷入局部最优。

- 禁忌(prohibited or restricted by social custom)指的是不能触及的事物。
- 禁忌搜索是由弗雷德·格洛弗于1986年提出，1989年加以形式化
- 它是一种元启发式(meta-heuristic)算法，用于解决组合优化问题
- 它使用一种局部搜索或邻域搜索过程，从一个潜在的解X到改进的相邻解X’之间反复移动，直到满足某些停止条件。
- 用于确定解的数据结构被称为禁忌表(tabu list)

禁忌搜索的三种策略：
1. 禁止策略
	控制何物进入该禁忌表
2. 释放策略
	控制何物以及何时退出该禁忌表
3. 短期策略
	管理禁止策略和释放策略之间的相互作用来选择试验解
4. 特赦策略
	禁忌搜索算法中，迭代的某一步会出现候选集的某一个元素被禁止搜索，但是若解禁该元素，则会使评价函数有所改善，因此我们需要设置一个特赦规则，当满足该条件时该元素从禁忌表中跳出。

禁忌搜索可解决的问题：
- 旅行推销员问题
- 旅行比赛问题
- 车间作业调度问题
- 网络加载问题
- 图着色问题
- 硬件/软件划分
- 最小生成树问题

## 三、优化算法

### 模拟退火搜索

- 引论：爬山法的从不下山策略导致其不完备性，而纯粹的随机行走，又导致效率的低下，把单纯的爬山法和纯粹的随机相结合就是以下的模拟退火算法的思想。
- ldea: 通过允许向不好的状态移动来避开局部最值点，但允许的频率逐渐降低，
	- 起源：冶金的退火原理。
	- 退火用于对金属和玻璃进行回火或硬化。
	- 将一个固体放在高温炉内进行加热，提升温度至最大值。在该温度下，所有的材料都处于液体状态，并且粒子本身随机地排列。随着高温炉内的温度逐渐冷却，该结构的所有粒子将呈现低能状态。

- 初始解
	使用随机选择启发式方法生成
- 相邻节点
	随机生成当前解的变异
- 接受条件
	相邻节点使情况：
	1. 改善，接受
	2. 变坏，则以概率P接受
- 停止判据
	解具有比阈值低的值。以达到迭代最大总次数

模拟退火搜索的性质：
- 可以证明: 如果T降低的足够慢，则模拟退火能以趋近于1的概率找到最优解。
- 广泛应用于VLSI布局问题，航空调度等领域。

## 四、CSP问题

### 约束满足问题定义

标准搜索问题：
- 状态是一个黑盒，通过后续函数，启发式函数和目标测试来访问

CSP:
- 状态由多个变量X_i定义，变量X的值域为 D_i
- 目标测试是由约束集来确定，约束指定变量子集允许的赋值组合

### 标准搜索形式化CSP（增量形式化）

CSP的特点：
- 呈现出标准的模式
- 通用的目标和后继函数
- 通用的启发函数

### 回溯搜索

- 变量赋值是可交换的,i.e.
- \[ WA = red then NT = green\]等同于\[NT = green then WA = red \]
- 每个结点只需要考虑给一个变量赋值
	b = d 所以d^n片叶子
- 采用单变量赋值的深度优先搜索称为回溯搜索

- 回溯改进——最大受限变量
- 回溯改进——最小约束值

前向检查：
- 较早探测到不可避免的失败并且在后面避免它
- 算法思想：只保留未赋值的变量的合法值
- 当任何变量出现没有合法可赋值的情况终止搜索

约束传播：
- 前向检查把已经赋值变量的信息传播给未赋值变量，但不能提早检测到所有失败
- 约束传播是将一个变量的约束内容传播到其它变量上的方法

弧相容：
- 弧X->Y是相容的当且仅当对于变量X的每一种赋值，变量y都存在一个合法赋值。
- 如果变量X删除了一个合法赋值，则它的邻居变量需要重新检查
- 弧相容原则能比前向检查更早检测到失败
- 可以在变量赋值前或后进行弧相容检查

局部搜索算法解决CSPs：
- 局部搜索应用到约束满足问题CSPs：
	- 采用完全形式化表示
	- 动作定义为：给变量重新赋值
- 变量选择：随机选择一个违反约束的变量重新赋值
- 赋值方案采用最少冲突启发式原则

## 五、联机搜索

- 目标：用最小耗散达到目标
- 耗散：整个路径的总耗散
	- 竞争率=实际代价同如果已知搜索空间的解路径代价的比值
	- 当智能体偶然达到死路径可能造成无限循环
- 一个联机智能体具有维护环境地图能力
	- 随着输入的感知更新地图
	- 并应用它确定下一个行动
- 同A\*算法的区别
	- 一个联机版的智能体只能扩展它在当前状态中的实际结点（物理结点）

算法分析：
- 最坏的情形是每个节点被访问2次
- 即使某节点已经很接近目标它还需走很远
- 一个联机迭代加深的方法可以解决这个问题
- 联机深度优先只有当行为是可逆的才能进行

## 六、小结

- 局部搜索：
	- 爬山法在完整状态形式化上进行操作
	- 局部束搜索法保持k个状态的轨迹
	- 禁忌搜索采用一种带约束的邻域搜索
- 优化算法：模拟退火在大搜索空间逼近全局最优解
- CSP问题：是一类特别的问题：状态是通过对变量集的赋值，目标测试是通过变量值的约束定义
- 联机问题：联机问题(online)在执行时，需要计算和行动交叉进行的动态环境问题

# 第四章 搜索技术——群体智能

## 一、群体智能概述

- 人工智能研究的核心理念是“模拟智能活动的特征和结果”
- 模拟“单一”智能体
	- 自动推理、知识图谱、搜索技术等
- 模拟群体智能
	- 从自然界中适者生存的客观规律中获得启发
	- 物种在时间和空间上通过调整自身行为、属性来适应环境，使之生存并不断发展、延续

- 群体智能（swarm intelligence）是指在集体层面表现出的分散的、去中心化的自组织行为
- 如蚁群、蜂群构成的复杂类社会系统，鸟群、鱼群为适应空气或海水而构成的群体迁移，以及微生物、植物在适应生存环境时候所表现的集体智能

群体智能应该遵循五条基本原则（MillonasMM在1994年提出）：
- 邻近原则，群体能够进行简单的空间和时间计算
- 品质原则，群体能够响应环境中的品质因子
- 多样性反应原则，群体的行动范围不应该太窄
- 稳定性原则，群体不应在每次环境变化时都改变自身的行为
- 适应性原则，在所需代价不太高的情况下，群体能够在适当的时候改变自身的行为　

群体智能的特点：
- 控制是分布式的，不存在中心控制
	- 即不会由于某一个或几个个体出现故障而影响群体对整个问题的求解
- 群体具有较好的可扩充性
	- 群体智能可以通过非直接通信的方式进行信息的传输与合作，因而随着个体数目的增加，通信开销的增幅较小
- 群体智能的实现方便，具有简单性
	- 群体中每个个体的能力或遵循的行为规则非常简单
- 群体具有自组织性
	- 群体表现出来的复杂行为是通过简单个体的交互过程突现出来的智能

群体智能的两个维度：
对物种适者生存能力的解读，可以分时间和空间两个维度来考虑
- 时间维度
	种群通过代际繁衍，经历千万年的发展，不断进化以适应环境，借鉴物种随时间进化的过程来求解问题
- 空间维度
	在同一时间，种群内的大量简单个体在限定条件下通过交互协作，使整体种群具有某种适应环境的能力，模拟这种群体行为产生的智能

典型的群体智能算法

| 群体智能算法 | 基本思想          |
| ------ | ------------- |
| 遗传算法   | 模拟生物种群进化      |
| 差分进化算法 | 模拟生物种群进化www   |
| 粒子群算法  | 模拟鸟类寻找食物      |
| 蚁群算法   | 模拟蚁群寻找食物      |
| 人工鱼群算法 | 模拟鱼类生活觅食的特性   |
| 萤火虫算法  | 模拟萤火虫在晚上的群聚活动 |

## 二、遗传算法

遗传算法基本思想：
- 模拟达尔文“优胜劣汰、适者生存”的原理
- 模拟孟德尔遗传变异理论
- 借鉴魏斯曼的自然选择理论

遗传算法基本流程：
1. 初始化种群
2. 计算个体适应度
3. 轮盘赌选择双亲进行繁殖
4. 基因交叉突变生成后代染色体
5. 重复繁殖直至种群替代原始种群
6. 评价是否有个体满足条件，若有停止算法，若无继续繁殖下一代

``` code
function GENETIC-ALGORITHM(population,FITNESS-FN) returns an individua
inputs: population, a set of individuals
		FITNESS-Fn, a function that measures the fitness of an individual
repeat
	new population < empty set
	for i= 1 to SIZE(population) do
		X<-RANDOM-SELECTION(population,FITNESS-FN)
		y<-RANDOM-SELECTION(population,FITNESS-FN)
		child<-REPRODUCE(x,y)
		if (small random probability) then child - MUTATE(child
		add child to new population
	population<-new population
until some individual is fit enough, or enough time has elapsed
return the best individual in population, according to FITNESS-FN
```

## 三、粒子群算法

粒子群算法基本思想：
- 设想这样一个场景：
	- 一群鸟在随机搜索食物
	- 在这块区域里只有一块食物
	-  所有鸟都不知道食物在哪里
	- 每只鸟都知道当前时刻离食物最近的鸟的位置
	-  它们能感受到当前的位置离食物还有多远

- 那么找到食物的最优策略是什么呢？
	- 搜寻目前离食物最近的鸟的周围区域

- 粒子群优化算法的基本思想是通过群体中个体之间的协作和信息共享来寻找最优解

粒子群算法流程：
1. 粒子位置和速度初始化
2. 粒子适应度值计算
3. 记录个体极值和群体极值
4. 速度更新和位置更新
5. 粒子适应度值计算
6. 个体极值和群体极值更新
7. 满足终止条件则结束，反之则继续更新

粒子群算法和遗传算法对比：
- 粒子群算法相比于遗传算法更加的简单易懂，其调节参数较少
- 不同点
	- PSO由于不需要编码，没有GA算法中由于编码带来的求解精度限制
	- 在信息共享方面，大多数情况下所有粒子可能比遗传算法中的进化个体以更快速度收敛
		- GA算法，染色体之间共享信息，所以整个种群的移动是比较均匀地向最优区域移动
		- PSO算法，粒子通过当前搜索到最优点进行共享信息，很大程度上这是一种单项信息共享机制，整个搜索更新过程是跟随当前最优解的过程
- 相似点
	- 都为仿生算法，求解过程中存在多个解，因此都可以进行并行运算
	- 对个体的适配信息进行搜索，因此不受函数约束条件的限制，如连续性、可导性等

``` code
For each particle
	Initialize particle
Do
	For each particle
		Calculate fitness value
		If fitness value > best fitness value (pBesr) in history
			set fitness value as new pBest
	Choose particle with best fitness value of all particles as gBest
	For each particle
		Calculate particle velocity
		Update particle position
While maximum iterations or minimum error criteria is not attained
```

## 四、蚁群算法

蚁群算法设计：
1. 模拟蚂蚁
2. 模拟环境：地图
3. 模拟蚂蚁选择路线
4. 模拟信息素释放和消散

蚁群算法流程：
1. 初始化
2. 更新禁忌表
3. 确定行走方向
4. 求信息素增量
5. 判断终止准则

# 搜索算法小结

- 局部搜索：爬山法在完整状态形式化上进行操作；局部束搜索法保持k个状态的轨迹；禁忌搜索采用一种带约束的邻域搜索。
- 优化与进化算法：模拟退火在大搜索空间逼近全局最优解；遗传算法模仿自然选择的进化过程。
- 群体智能：蚁群优化可以寻找图的最好路径；粒子群优化通过迭代来改善一个候选解。

# 第五章 对抗搜索和博弈

## 一、博弈

### 博弈的分类

- 参与者或玩家 (player) 合作博弈与非合作博弈
	- 合作博弈（cooperative game）：部分参与者可以组成联盟以获得更大的收益
	- 非合作博弈（non-cooperative game）：参与者在决策中都彼此独立，不事先达成合作意向
- 静态博弈与动态博弈
	- 静态博弈（static game）：所有参与者同时决策，或参与者互相不知道对方的决策
	- 动态博弈（dynamic game）：参与者所采取行为的先后顺序由规则决定，且后行动者知道先行动者所采取的行为
- 完全信息博弈与不完全信息博弈
	- 完全信息（complete information）：所有参与者均了解其他参与者的策略集、收益等信息
	- 不完全信息（incomplete information）：并非所有参与者均掌握了所有信息

### 搜索与对抗搜索


| 搜索                       | 对抗搜索                       |
| ------------------------ | -------------------------- |
| 单智能体                     | 多智能体                       |
| 解是寻找目标的（启发式）方法           | 解是策略（指定对每个可能对手回应的行<br>动策略） |
| 启发式法可以找到最优解              | 时间受限被迫执行一个近似解              |
| 评价函数：给定节点从起始到目标的代价<br>估计 | 评价函数：评估博弈局势的“好坏”           |

## 二、博弈中的优化决策

### 极大极小算法思想

- 轮到MAX走时，从当前状态扩展博弈树到所设置深度h（根据允许时间要求所能的深度）
- 计算每个叶子节点的评估值；
- 根据不同的叶子节点采用不同的方法倒推计算各节点值；
	- 对于MAX节点选取其后继节点中最大的值为其评估值；
	- 对于MIN节点选取其后继节点中最小的值为其评估值。
- 选择移动到具有最大倒推值的MIN节点。

最大最小算法表现为博弈树的深度优先搜索。
- 优点：
	- 算法是一种简单有效的对抗搜索手段
	- 在对手也“尽力而为”前提下，算法可返回最优结果
- 缺点：
	- 如果搜索树极大，则无法在有效时间内返回结果
- 改善：
	- 使用alpha-beta pruning算法来减少搜索节点
	- 对节点进行采样、而非逐一搜索 (i.e.,MCTS)

## 三、Alpha-Bata剪枝

### 为何称其为Alpha-Beta

- 剪枝从如下两个参数得到其名称：
	- α：沿着MAX路径上的任意选择点，迄今为止我们已经发现的最高值。
	- β：沿着MIN路径上的任意选择点，迄今为止我们已经发现的最低值。
- 搜索依次完成如下动作：
	- 边搜索边更新α和β的值，并且
	- 一旦得知当前节点的值比当前MAX或MIN的α或β值更差，则在该节点剪去其余的分枝。

### 如何做得更好？

- 假设博弈树的分支因子为b，则极大极小算法检测O(b^h) 个节点，最坏情况下Alpha - Bata剪枝也是一样的。
- 以下情况下Alpha - Bata剪枝比极大极小算法优异：
	1. 一个MAX节点的MIN孩子们是按降序排列的。
	2. 一个MIN节点的MAX孩子们是按升序排列的。
	- 这种情况下Alpha - Bata剪枝算法检测O(b^h/2) 个节点。[Knuth and Moore, 1975]
	-  但这需要一个神谕（如果我们知道节点的完美排序，我们就不需要搜索博弈树了）。
- 如果节点是按随机排序的，那么Alpha - Bata剪枝算法检测O(b^3h/4) 个节点。

- 启发式极小极大值：节点的启发式排序
	- 根以下的节点排序依照前一次循环所得到的倒推值进行。

其他改进方法
- 采用适应深度限制+循环加深
- 扩大搜索范围：保留k>1条的路径，代替仅保留一条，并且在大于设置深度的叶子节点下扩展博弈树（帮着对付水平线效应——指当前的后继都是差不多的状态）。
- 特别情况扩展：在设置深度h时，如果一个节点明显地比其他的节点好，则沿着这个移动继续扩展几步。
- 用对照表法对付重复状态。
- 另外还有诸如当 α-β 虽然不成立，但 α 不比 β 小多少时，仍然采用剪枝，特别是在开局初期。

## 四、蒙特卡洛树搜索

### 关于蒙特卡罗方法

- 蒙特卡罗方法是一大类计算算法，它凭借重复随机采样来获得数值结果。
- 它们往往遵循如下特定模式:
	- 定义一个可能的输入域
	- 从该域的一个概率分布随机地生成输入
	- 对该输入进行确定性计算
	- 将结果聚合

### 蒙特卡洛树搜索（MCTS）

- MCTS将蒙特卡罗仿真与博弈树搜索相结合。
- 和minimax一样，每个节点对应于一个的博弈状态。
- 不同于minimax，节点的值通过蒙特卡罗仿真来估值。

### 蒙特卡洛树规划

- 单一状态蒙特卡洛规划： 多臂赌博机 (multi-armed bandits)
- 上限置信区间策略 (Upper Confidence Bound Strategies, UCB)
- 蒙特卡洛树搜索 (Monte-Carlo Tree Search)
	- UCT (Upper Confidence Bounds on Trees)

## 五、博弈的发展情况

- 西洋跳棋: Chinook 在1994年打败了人类冠军 Marion Tinsley 。
- 黑白棋： The Logistello software 在1997年6：0完败世界冠军。
-  国际象棋: Deep Blue 1997打败了人类冠军 Garry Kasparov.
- 围棋: 2016年7月18日，GoRatings公布最新世界排名，AlphaGo以3612分，超越3608分的柯洁成为新的世界第一。
- 2017年10月19日国际学术期刊《自然》（Nature）阿尔法元100：0战胜哥哥阿尔法狗

# 第六章 机器学习——机器学习视角

## 一、什么是机器学习

机器学习是人工智能的一个分支，从事构建和研究可以从数据中学习的系统。

### 人工智能与机器学习

- 人工智能研究感知外部环境并为某个目标采取行动的“智能体”。
- 机器学习是人工智能的一个分支，从事构建可以从数据中学习的系统。

### 与其他学科的关系

- 统计学习
- 模式学习
- 数据挖掘
- 计算机视觉

### 人类学习和机器学习

- 人类学习从观察中积累经验来获取技能
- 机器学习从数据中积累或计算的经验获取技能


### 什么是机器学习的技能

- 技能改善某些性能指标
- 机器学习可以通过从数据中学到的经验来改善某些性能指标

### 两种通用的学习类型

- 归纳学习
	- 从特定的训练实例中获得或发现通用的规则或事实
- 演绎学习
	- 使用一套一致的规则和事实去推导适合该训练数据的猜测

### 机器学习的若干定义

- 研究给予计算机学习能力而不必显示地编程的领域
- 一种系统用它来改善其性能的过程
- 运用示例数据或经验的计算机程序来优化性能指标
- 运用经验来改善性能或做出正确预测的计算方法

### 机器学习的形式化定义

一个能根据某类任务T和性能指标P中的经验E学习的计算机程序，如果它在任务T中的性能P度量随经验E而改善。

### 形式化定义的三要素

- 任务
- 表现
- 经验

## 二、机器学习的历史

略

## 三、为什么不同的视点

### 理解机器学习的难点

- 应该选择哪种机器学习算法
- 机器学习算法有成千上万种之多
- 一个视点是不够的

### 机器学习如何工作

使用经验或与环境交互来改善性能，或做出精准预测

## 四、机器学习的三种视点

### 为什么有三个视点

- 做什么——问题——学习任务
- 什么形式——场景——学习范式
- 如何做——手段——学习模型

### 三个视点的定义

| 类型   | 描述                |
| ---- | ----------------- |
| 学习任务 | 表示可以用机器学习解决的基本问题  |
| 学习范式 | 表示机器学习中发生的典型场景    |
| 学习模型 | 表示可以处理完成一个学习任务的方法 |

### 学习任务

学习任务用于表示可以用机器学习的基本问题。

- 计算机视觉
- 模式识别
- 自然语言处理

机器学习中的典型任务：
- 分类
- 回归
- 聚类
- 排名
- 密度估计
- 降维
- 优化

### 学习范式

学习范式用于表示机器学习中发生的典型场景。

区分学习范式根据机器学习的典型场景或样式：
- 它怎样从数据中学习
- 它如何同环境互动

机器学习中的典型范式：
- 有监督
- 无监督
- 强化

### 学习模型

学习模型用于表示可以处理完成一个学习任务的方法。

机器学习的效果很大程度上取决于解决该学习任务时所选用的方法。

机器学习的代表性模型：
- 几何
- 逻辑
- 网络
- 概率

## 五、应用和术语

### 机器学习的应用领域

- 机器感知
- 计算机视觉
- 视频分析
- 模式识别
- ......

### 机器学习的术语

- 样本
- 特征
- 标记
- 训练样本
- 验证样本
- 测试样本
- 损失函数
- 假设集
- 抽象
- 泛化
- ......

# 第六章 机器学习——机器学习任务

## 学习任务

学习任务用于表示可以用机器学习解决的基本问题。

为什么研究学习任务：应用中会产生各种类型的问题。

机器学习中的典型任务：
- 分类
- 回归
- 聚类
- 排名
- 密度估计
- 降维
- 优化、

## 机器学习中的任务

### 分类

#### 什么是分类

- 分类是基于包含已知类别成员观测值的训练数据集、来辨识新的观测值属于哪一组类别的任务。
- 解决输出被分为两个或多个类别的问题
- 为每个项指定一个类别

#### 分类器

一种实现分类、尤其是构成一种具体实现的算法，被称为一个分类器。

#### 分类：训练

对已知类别的标注数据进行训练得到分类函数。

#### 分类：实测

对未知数据用分类函数进行分类。

#### 一种分类的形式化描述

设目标标注函数：
$$ f:X->Y$$

训练集（标注的训练样本集）：
$$S=\{(x_i, y_j)|(x, y)\in X*Y, i\in [1, m], j\in [1, n]\}$$

回归算法，给定假设集H，来决定一个假设（回归函数）：
$$h:X->Y and\ h\in H$$

具有小的泛化错误：
$$R(h)=Pr_x[L(h(x)\neq f(x))]$$

给定一个未知类别的实测数据集：
$$X=\{x \in X, i\in [1,m]\}$$

使用前面训练好的分类函数来预测结果：
$$Y = h(X)=\{y_i|y\in Y, j\in [l,n], h(x) = y\}$$其中Y是已知类别的集合。

#### 线性分类

线性分类是通过线性分类器来进行分类。

一个线性分析器是具有一个线性决策边界的线性判别函数。

#### 非线性分类

非线性分类是通过一个非线性分类器来进行分类。

一个非线性分类器具有若干非线性决定边界，并且可能是非连续决定边界。

#### 维度

如果空间的维度为n，则它的线性分类器的维度为n-1的超平面。

#### 类别

$y_k(x)=w_k+b$

二元分类：k=2
多元分类：k>2

#### 分类的典型应用

- 计算机视觉
- 模式识别
- 生物特征识别
- 统计自然语言处理
- 文档分类
- 互联网搜索引擎
- 信用评分

#### 分类的典型算法

- AdaBoost
- 决策树
- 人工神经网络
- 贝叶斯网络
- 隐马可夫模型
- K-近邻
- 核方法
- 线性判别分析
- 朴素贝叶斯分类器
- Softmax
- 支撑向量机（SVM）

### 回归

##### 什么是回归

- 回归分析是估计变量间关系的统计过程。它包含着对多变量进行建模与分析的许多技术，其焦点是某个自变量与一个或多个因变量的关系。
- 要解决的是输出是真实连续值的问题。
- 预测每个项的真实值。

#### 回归与分类

- 相似性
	- 需要训练过程
- 差异性
	- 回归输出是一个真实连续值
	- 分类输出是一个离散的类别

#### 回归：训练

对已知类别的标注数据进行训练得到回归函数。

#### 回归：实测

对未知数据用分类函数进行预测。

#### 一种回归的形式化描述

设目标标注函数：
$$ f:X->Y$$

训练集（标注的训练样本集）：
$$S=\{(x_i, y_i)|(x, y)\in X*Y, i\in [1, m]\}$$

回归算法，给定假设集H，来决定一个假设（回归函数）：
$$h:X->Y and\ h\in H$$

具有小的泛化错误：
$$R(h)=E_x[L(h(x), f(x))]$$

回归

给定一个未知输出的实测数据集：
$$X=\{x_i|x \in X, i \in [1, m]\}$$

使用前面训练好的回归函数来预测回归结果：
$$Y = h(X) = \{y_i | y \in Y, i \in [l, n], h(x)=y\}$$

其中，Y是一个真实连续数值的集合。

#### 线性回归

线性回归中，采用具有如下特征的函数对观测数据进行建模：
- 该函数时模型参数的线性组合
- 该函数取决于一个或多个独立变量

#### 非线性回归

非线性回顾中，采用具有如下特征的函数对观测数据进行建模：
- 该函数时模型参数的非线性组合
- 该函数取决于一个或多个独立变量

#### 回归的典型应用

- 趋势估计
- 传染病学
- 金融
- 经济
- 环境科学

#### 回归的典型算法

- 贝叶斯线性回归
- 百分比回归
- 核岭回归
- 支撑向量回归
- 分位数回归
- 回归树
- 级联相关
- 分组方法数据处理
- 多元自适应回归样条
- 多线性插值

### 聚类

#### 什么是聚类

- 聚类是以这样的一种方式将对象进行分组的任务，即同一组中的对象彼此之间比其他组中的对象更相似。
- 将对象进行分组的过程，组内成员具有某种方式的相似性。
- 将数据对象进行分组。

#### 聚类与分类

- 相似性
	- 分组
- 差异性
	- 聚类给输入对象标识相似的组，分类给输入项分派预定义的类。
	- 聚类没有训练数据，分类有训练数据。
	- 聚类基于距离、密度等发现类聚，分类的分类器需要具有较高的分类精度。

#### 聚类实现

将无标注的输入数据根据聚类算法进行聚类分析得到标识类聚，再进行聚类验证。

#### 一种聚类的形式化描述


设聚类函数：
$$ h:X->Y\ and\ h \in H$$

给定一个未知聚类的测试集：
$$X=\{x_i|x \in Y, i\in [1, m]\}$$

采用上述确定的聚类函数来分析聚类结果：

$$Y = h(X) = \{y_i | y \in Y, i \in [1, n], h(x)=y\}$$

#### 聚类算法的典型方法
- 基于连接性聚类
- 基于中心点聚类
- 基于分布聚类
- 基于密度聚类

#### 聚类的典型应用

- 医学
- 商务和营销
- 万维网
- 计算机科学

#### 聚类的典型算法

- k-means
- k-modes
- PAM
- CLARA
- FCM
- BIRCH
- CURE
- ROCK
- Chamelemon
- Echidna
- DBSCAN

### 排名

#### 什么是排名

- 排名是一组项之间的关系，即对于任意两个项，满足第一个“排名高于”、“排名低于”或“排名等于”第二个，
- 排名是一种数据转换，其中数值或者顺序值由其排名来代替。
- 依据某种准则整理数据项。

#### 一种排名的形式化描述

目标排名函数：
$$f:X*X->Y=\{-1, 0, +1\}$$

其中，
$$若x排名高于x',f(x,x')=+1$$
$$若x排名低于x',f(x,x')=-1$$
$$若x排名等于x',f(x,x')=0$$

训练数据：
$$S=\{(x_i,x'_i,y_j)|y_i=f(x_i,x'_i)\in Y,i\in [1,m],j\in [1,3]\}$$

给定一个将X\*X映射到Y={-1,0+1}的假设函数集H，选择一个具有目标函数f的假设h∈H：
- 最小预期泛化错误：$$R(h)=Pr_(x,x')[f(x,x')\neq 0 \bigwedge(y_i(h(x'_i)-h(x_i))\leq0)] $$
- 经验性成对误排名错误：$$\hat R(h)=1/m \sum_{i=1}^{m} 1((y_i \neq0)\bigwedge(y_i(h(x'_i)-h(x_i))\leq0)) $$

#### 典型的排名方法
- 基于分值方法
- 基于偏好方法

#### 排名的典型应用
- 信息检索领域
- 其他领域

### 降维

#### 什么是降维

- 将初始的较高维数据表示转换为这些数据的低维表示，而保留原始表示的某些性质
- 通过将高维空间映射到低维空间表示来简化输入
- 将输入映射到低维空间

#### 为什么降维

- 维度灾难
- 数据稀疏或无关
- 可视化

#### 线性与非线性

- 线性降维
- 非线性降维

#### 线性降维的典型方法

- 主成分分析（PCA）
- 线性判别分析（LDA）
- 多线性子空间学习
	- 多线性主成分分析（MPCA）
	- 多线性线性判别分析（MLDA）

#### 非线性降维的方法
- 多元尺度分析
- 核方法
- 流形学习方法

#### 降维的典型应用

- 图像识别
- 人脸识别
- 手写体识别
- 基因表达谱
- ......

# 第六章 机器学习——机器学习范式

## 什么是学习范式

学习范式用于表示机器学习中发生的典型场景。

## 怎样区分学习范式

根据机器学习的典型场景或样式：
- 它怎样从数据中学习
- 它如何同环境互动

### 机器学习是如何工作的

1. 训练数据经过学习算法进行训练得到假设函数
2. 未知数据经过假设函数得到输出
3. 输出的结果再反馈回假设函数

## 机器学习中的典型范式


| 范式  | 简短描述                          | 典型算法  |
| --- | ----------------------------- | ----- |
| 有监督 | 算法采用一组标注好的数据进行训练，再对所有的为支点做出预测 | 支撑向量机 |
| 无监督 | 算法仅接受未标注的数据，再对所有的未知点做出预测      | k-均值  |
| 强化  | 算法与外部环境交互，每个动作得到一个回报          | Q-学习  |

### 有监督学习

#### 什么是有监督学习

- 智能体接受一组标注的样本作为训练数据，然后对所有的未知点进行推测。
- 这种方式试图生成从输入到输出的函数或映射，然后可以用于对预先未知的数据生成输出。
- 有监督学习中的训练数据：
	- 每个训练数据具有一个已知标注作为输入数据。
	- 标注是由输入对象和预期输出值组成的对。
- 训练后的假设函数：
	- 可用于映射新的位置数据。

#### 有监督学习的6个步骤

1. 确定训练类型
2. 手机训练集
3. 确定特征提取方法
4. 设计该任务的算法
5. 训练该算法
6. 评估其精确性

#### 与有监督学习相关的任务

- 分类
- 回归
- 排名

#### 数学描述

- 由总体（X，Y）给定N个样本：{(x_i, y_i)}，i = 1,2, ..., N，其中$$x_i=(x_{i1}, x_{i2}, ..., x_{id})^T$$为d维向量，表示d个特征，其中的分量x_ij表示第i个样本第j个特征的取值。
- 计算机自动寻找一个决策函数（模型）f来建立X和Y之间的关系$$\hat Y=f(X,\theta )$$
- 设计合适的算法，使得Y^hat与Y之间的差别尽可能小

#### 训练和评估

- 训练集：训练模型
- 测试集：评估模型

#### 有监督学习的流程
1. 将数据集划分为训练集和测试集
2. 使用训练集训练模型
3. 使用测试集评估模型效果

#### 回归和分类

- 回归
	- 预测标签为连续型数值变量
	- 样本为{(x_i, y_i)}，i=1, 2, ..., N，特征x_i∈R^d（d维实数向量），目标y_i∈R，决策函数f：R^d->R
	- 根据房屋信息对房屋价格进行预测
- 分类
	- 预测标签维离散型数值变量
	- 二分类任务中，样本为{(x_i, y_i)}，i=1, 2, ..., N，特征x_i∈R^d（d维实数向量），目标y_i∈{-1, +1}，决策函数f:R^d->{-1, +1}
	- 垃圾邮件识别任务，将邮件分类为正常邮件和垃圾邮件

#### 回归模型

- 线性回归
	- 一元线性回归
	- 多元线性回归
- 多项式回归
- 回归树
- 支持向量回归

#### 分类模型

- 逻辑回归
- K-近邻
- 决策树
- 支持向量机

![Pasted image 20240421212628](../images/人工智能/Pasted%20image%2020240421212628.png)

![Pasted image 20240421212646](../images/人工智能/Pasted%20image%2020240421212646.png)

![Pasted image 20240421212706](../images/人工智能/Pasted%20image%2020240421212706.png)

![Pasted image 20240421212725](../images/人工智能/Pasted%20image%2020240421212725.png)

![Pasted image 20240421212736](../images/人工智能/Pasted%20image%2020240421212736.png)

![Pasted image 20240421212745](../images/人工智能/Pasted%20image%2020240421212745.png)

![Pasted image 20240421212754](../images/人工智能/Pasted%20image%2020240421212754.png)

#### 支持向量机

![Pasted image 20240421212906](../images/人工智能/Pasted%20image%2020240421212906.png)

![Pasted image 20240421212950](../images/人工智能/Pasted%20image%2020240421212950.png)

![Pasted image 20240421212958](../images/人工智能/Pasted%20image%2020240421212958.png)


#### 有监督学习模型评估

通过模型的评估可以得知模型性能的优劣，从而选择出最优模型

##### 回归模型评价指标

- 均方误差（MSE）：$MSE=\frac{1}{n} \sum_{i=1}^{n}(y_i - \hat y_i)^2$
- 其中y_i为真实值，y^hat_i为预测值
- MSE越小，预测模型越好

##### 分类模型评价指标

混淆矩阵：

| 真实\预测 | 正类  | 负类  |
| ----- | --- | --- |
| 正类    | a   | b   |
| 负类    | c   | d   |

- 正确率$P=\frac{a+d}{a+b+c+d}$，取值范围为[0,1]，正确率越高，模型越好
- 错误率$P'=\frac{b+c}{a+b+c+d}$，取值范围为[0,1]，错误率越低，模型越好
- 召回率$P_1=\frac{a}{a+b}$
- 精确率$P_2=\frac{a}{a+c}$

#### 典型的分类与回归算法


| 算法     | 任务类型 | 预测精度 | 训练速度 |
| ------ | ---- | ---- | ---- |
| 自适应增强  | 两者   | 高    | 慢    |
| 人工神经网络 | 两者   | 高    | 慢    |
| k近邻    | 两者   | 低    | 快    |
| 线性回归   | 回归   | 低    | 快    |
| 逻辑回归   | 分类   | 低    | 快    |
| 朴素贝叶斯  | 分类   | 低    | 快    |
| 决策树    | 两者   | 低    | 快    |
| 随机森林   | 两者   | 高    | 慢    |
| 支撑向量机  | 两者   | 高    | 慢    |

#### 有监督学习的应用

- 计算机视觉中的物体识别
- 光学字符识别
- 手写体识别
- 信息检索
- 学会排名
- 垃圾邮件检测
- 语音识别
- 生物信息学
- 化学信息学

#### 有监督学习的变体

- 半监督学习
- 弱监督学习
- 一次性学习
- 零次性学习

### 无监督学习

#### 什么是无监督学习

- 智能体专门接受未标注数据，并对所有的未知点做出预测。
- 其目标是发现数据中共性的东西，或者减少正在考虑的随机变量的数量。

#### 有监督学习与无监督学习


| 有监督学习         | 无监督学习         |
| ------------- | ------------- |
| 给予学习器的样本是已标注的 | 给予学习器的样本是未标注的 |
| 样本用于训练该算法     | 没有训练过程        |

#### 与无监督学习相关的任务

- 聚类
- 密度估计
- 降维

#### 降维和聚类

##### 聚类

- 根据某种“相似程度”或者“距离”，把距离近或者相似度高的样本归到一类
- 样本为{x_i}，i=1, 2, ..., N，特征x_i∈R^d（d维实数向量），决策函数f:R^d->{C_1, C_2, ..., C_k}，C_1, C_2, ..., C_k为k个不同的类别，称为k个簇
- 如根据客户的不同消费行为，将客户分为不同的群体

##### 降维

- 将数据的特征维度从高维转换到低维

#### K-Means

- 基本思想
	- 将n个样本划分到k个簇中，其中每个样本归属于距离自己最近的簇，使簇内具 有较高的相似度，而簇间的相似度较低。
- 算法步骤
	- 随机选择k个样本作为初始中心
	- 重复迭代如下步骤直至收敛：
		- 把每个样本指派到最近的中心，形成k个簇
		- 重新计算每个簇的中心
		- 直到中心不在发生变化

#### 层次聚类

- 基本思想
	- 在不同层级上对样本进行聚类，逐步形成树状结构
- 两种常用形式
	- 自顶向下 
	- 自底向上
- 其中，自底向上的主要做法是，在开始时，将每个样本视为一个簇，重复的合并最近的两个簇，直到簇的个数达到给定值。

- 初始样本都在叶子节点
- 每一步选择两簇进行合并，分支的高度代表聚合的距离
- 根节点表示包含所有样本的簇
- 在任意的高度将树截断都可以得到一组聚类结果

层次聚类簇间距离的计算
- 完整连接法（ complete）：两簇之间最远的样本之间的距离
- 平均连接法（ average）： 两簇间所有样本对的距离的平均值 
- 单连接法（ single）：两簇之间最近的样本之间的距离
- 离差平方和（ward）：两簇的离差平方和之和

#### 聚类的主要方法

- 根据密度峰值聚类
	- 类聚中心点的特性是
		1. 密度高于其相邻点
		2. 距离大于其它较高密度点
	- 特点：
		1. 直观得到类聚的个数
		2. 自动地发现和排除离群点
		3. 无论其形状以及空间的维度，类聚都能被识别到

#### 降维

##### 降维的目的

- 摒弃冗余信息，删除噪音
- 降低计算时间、空间复杂度
- 简单模型具有更好的鲁棒性
- 实现数据可视化
- 可以作为特征提取的方法

##### 降维方法

- 主成分分析
- 非负矩阵分解
- 等度量映射

#### 主成分分析

基本思想：构造原始特征的一系列线性组合形成低维的特征，以去除数据的相关性，并使降维后的数据最大程度地保持原始高维数据的信息。

#### PCA算法流程

![Pasted image 20240421220621](../images/人工智能/Pasted%20image%2020240421220621.png)


#### 无监督学习的应用

- 将数据区分成若千个组( 类聚 )
- 发现数据中的规律从而得到有价值的信息
- 将高维数据映射到低维空间

### 强化学习

#### 什么是强化学习

- 在强化学习中，其学习器是一个决策制定智能体，在环境下采取行动并获得这些动作的回报。
- 经过一系列试错运行之后，该智能体能够学到最优策略。
- 该策略是经过一个阶段的动作以及与环境的交互之后，使其回报最大化。
- 强化学习的灵感来自行为心理学
- 关注于智能体如何在环境中采取行动，为了使累积回报最大化。

#### 强化学习的形式化

- 强化学习包含：
	- 一组智能体的状态，s_t∈S
	- 一组智能体的动作，a_t∈A
	- 一个从状态到动作的转换函数，T(s_t, a_t, s_t+1)
	- 一个回报函数，R(s_t, a_t, s_t+1)
- 寻找一个策略，π(s_t)
- 尚未知道T或R
	- 即，不知道哪个状态好或者要做什么动作
	- 必须实际去尝试要学习的行动或状态

#### 强化学习的类型

- 基于模型——构建环境的模型
	- 首先以马可夫决策过程方式动作，并学习T和R。
	- 然后用学习的T和R进行数值迭代或策略迭代。
- 无模型——学习策略而没有任何模型
	- 避开学习T和R的过程，采用直接评估策略。
	- 基于预测的时间差分(TD)法。

#### Q-Learning

![Pasted image 20240422140457](../images/人工智能/Pasted%image%2020240422140457.png)

![Pasted image 20240422140708](../images/人工智能/Pasted%20image%2020240422140708.png)

![Pasted image 20240422143704](../images/人工智能/Pasted%20image%2020240422143704.png)

![Pasted image 20240422151642](../images/人工智能/Pasted%20image%2020240422151642.png)


Q学习开始的第一个片段的学习，从前向后的迭代，是将涉及到的所有的Q(s,t)后面的Q(s’,a’)都向前传了一步，当然,最后面最终目标的高奖励100也打折扣0.8向前传了一步。

第二个片段的迭代，如果涉及了前面被传的值函数，会把最高奖励100以折扣0.82又向前传一步。由于迭代的初始值以及状态动作的选择都是随机，而且迭代的片段又非常多，所以经过多个片段的多次迭代，会把后面总目标的高奖励100，打折扣分层次的传到前面各个状态各个动作的Q值中去。

因为向前的传播有折扣0.8<1，所以离总目标越近的状态动作，往往Q值越大（折扣少），离总目标远，因为折扣次数多，Q值一般将越小。由单调性，一般Q值越大的状态-动作，离总目标也越近。因此，值函数Q(s,a)在一定意义下，反映了状态s下动作a与总目标的接近程度。

所以，在Q表训练完成之后，智能体进行决策时，每步都选Q值最大的动作，就会简洁快速走到目标状态。

这个参数r<1的小于1很重要，它不但把各个Q值分出层次，而且使Q表经过多次迭代会渐渐的收敛。

Q学习算法设计的非常巧妙，看似迭代的每一步都只是将后一步的Q值传到前一步，但经过多次的迭代后，最终会将总目标的高奖励，由大到小分层次的传到前面的各个状态的各个动作，表示出各个状态各个动作与最终目标的接近程度。

有意思的是，计算Q值使用贝尔曼方程进行迭代，是一步步的由前往后走（向最终目标），而实际效果是将后面（包括最终目标）的Q值一步步的往前传。

#### 强化学习的典型应用

- 机器人
- 计算机游戏

#### 三种范式之比较

| 学习范式  | 特点                        |
| ----- | ------------------------- |
| 有监督学习 | 通过标注数据提供输入和输出对，从样本中学习     |
| 无监督学习 | 发现无标注数据集中隐藏的结构，自我学习       |
| 强化学习  | 不提供输入和输出对，专注于在线的性能优化，在线学习 |

#### 学习任务和范式的关系

学习范式
- 有监督学习
	- 分类
	- 回归
	- 排名
- 无监督学习
	- 聚类
	- 密度估计
	- 降维
- 强化学习
	- 优化

#### 机器学习中的其它范式

| 范式    | 简介                            |
| ----- | ----------------------------- |
| 集成学习  | 将多个弱学习器组成一个强学习器               |
| 学会学习  | 基于先前的经验学习归纳偏差                 |
| 迁移学习  | 将已有的知识用于不同但相关的问题              |
| 对抗式学习 | 以一种对抗性方式（即零和博弈）来生成模仿某种分布的数据   |
| 协同式学习 | 以某种协同式（如非聚和博弈，双赢）来得到所期待的输出结果。 |

# 第六章 机器学习——机器学习模型

## 什么是学习模型

学习模型用于表示可以求解一个学习任务的方法。

它是属于算法层面而不是数据层面的模型（方法）。

## 为什么要研究学习模型

机器学习的效果在很大程度上取决于解决该学习任务时所选用的方法。

## 机器学习的代表性模型

![Pasted image 20240422153758](../images/人工智能/Pasted%20image%2020240422153758.png)


## 几何模型

### 什么是几何模型

- 几何模型直接在实例空间构建。空间可以被认为是几何的概念，例如：欧氏几何、黎曼几何
- 二维或三维的几何模型易于可视化
- 可能适用于高维空间的集合概念通常冠以hyper（超），例如：超平面

### 线与面

使用线或平面这样的几何模型来构建一些机器学习算法。

### 流形

- 什么是流形
	- 一种拓扑空间，其每个点附近存在近似的欧几里得空间
	- 更精确地说，一个n维流形的每个点都有一个相邻点，它与维度为n的欧几里得空间同胚
- 数据的低维结构
	- 高维输入数据中可以发现其固有的低维特性
- 低维流形假设
	- 高维输入数据依附于一个低维流形

#### 典型的流形学习的算法

- 等距映射
	- 一种非线性降维的全局几何框架
- 局部线性嵌入
	- 凭借局部线性嵌入的非线性降维
- 拉普拉斯特征映射
	- 用于降维和数据表达的特征映射
- 局部切空间对齐
	- 主要流形与采用局部切空间对齐的非线性降维
- 归纳流形学习
	- 采用结构化支撑向量机的归纳流形学习

#### 典型的流形学习应用

- 图像处理
- 基因表达图谱
- 手写体识别
- 人脸识别

## 逻辑模型

### 什么是逻辑模型

逻辑模型被定义为易于解释的逻辑表达式，或者易于转换成人类能够理解的规则。

### 典型的逻辑模型

- 一阶逻辑
- 关联规则
	- 是一种用于在大型数据库中变量之间发现有趣关系的方法
	- 它采用差异趣味性度量方式，旨在识别数据库中发现的强规则
- 决策树
	- 使用决策树作为预测模型，将项的观察结果映射到关于项的目标值
	- 目标变量可以取一组有限值的树模型称为分类树，其中叶节点表示类标签，分支表示通往这些类标签的特征连接

## 网络化模型

### 什么是网络化模型

- 特指人工神经网络模型（ANN）
- 一个ANN是人脑的一种人工表征，试图模拟人类的学习过程
- ANN可以通过互联的“神经元”构建一个系统，神经元之间相互发送消息
- 神经元之间的连接具有数值权重，可以通过经验调整，使ANN适应输入并且能够学习

![Pasted image 20240422160524](../images/人工智能/Pasted%20image%2020240422160524.png)

![Pasted image 20240422160534](../images/人工智能/Pasted%20image%2020240422160534.png)


### 神经网络模型的结构

- 前馈神经网络
	- 信息从输入结点仅仅以一个方向，即前进方向，穿过隐藏层并抵达输出节点
- 循环神经网络
	- 连接形成有向循环
	- 建立网络的内部状态，使之展现动态的时间特性

### 反向传播

即反向误差传播
- 第一阶段：传播
	- 前馈传播
		- 输入的训练数据穿过神经网络，从而生成输出激活值
	- 反向传播
		- 输出激活再使用训练数据目标穿过神经网络，生成所有的输出层和隐藏层神经元的差值
		- 差值=期待输出-实际输出
- 第二阶段：权值更新
	- 对每个权值：
		- 将其输出差值与输入激活相乘，以便得到该权值梯度
		- 从权值中减去梯度的比值( 百分比 )。该比值被称为学习率
			- 比值越大，神经元训练越快
			- 比值越低，训练精度越高

### 浅层与深层神经网络

- 深度神经网络的隐藏层大于2
- 超深度神经网络隐藏层大于10

### 深度神经网络

- DNNs使用许多层非线性处理单元，用于特征提取和转换
- 能够学习数据的多层特征或表征。高层特征来自于低层特征
- 成为更广泛的机器学习领域的一部分：学习数据表征
- 学习多层级表征，对应于不同的抽象层级，这种层级形成了一种概念的层次结构

代表性深度神经网络：
- 深度信念网络（DBN）
- 卷积神经网络（CNN）
- 深度波兹曼机（DBM）
- 循环网络（RNN）
- 长短期记忆（LSTM）
- 自动编码器
- 生成对抗网络（GAN）


![Pasted image 20240422163645](../images/人工智能/Pasted%20image%2020240422163645.png)

![Pasted image 20240422163721](../images/人工智能/Pasted%20image%2020240422163721.png)


### 深度神经网络的主要应用

- 语音识别
- 物体识别
- 图像检索
- 图像理解
- 自然语言处理
- 推荐系统
- 药物发现
- 生物医学信息学

## 概率模型

### 什么是概率模型

- 概率模型是采用概率论来表示所有不确定性的形式
- 贝叶斯规则允许我们推断未知量，适配我们的模型，做出预测并从数据中学习

### 典型的概率方法

- 朴素贝叶斯和贝叶斯网络
- 概率规划
- 高斯过程
- 隐藏马可夫模型

### 贝叶斯规则

- 贝叶斯规则告诉我们如何从数据推断假设
- 学习和预测可以看作是推理的形式
- 贝叶斯规则允许我们
	- 推断未知的量
	- 适配我们的模型
	- 做出预测
	- 从数据中学习

### 贝叶斯机器学习

![Pasted image 20240422164551](../images/人工智能/Pasted%20image%2020240422164551.png)

![Pasted image 20240422171814](../images/人工智能/Pasted%20image%2020240422171814.png)

### 马尔科夫决策

![Pasted image 20240422171851](../images/人工智能/Pasted%20image%2020240422171851.png)

![Pasted image 20240422171905](../images/人工智能/Pasted%20image%2020240422171905.png)

![Pasted image 20240422171918](../images/人工智能/Pasted%20image%2020240422171918.png)

![Pasted image 20240422171929](../images/人工智能/Pasted%20image%2020240422171929.png)

![Pasted image 20240422171936](../images/人工智能/Pasted%20image%2020240422171936.png)

![Pasted image 20240422171948](../images/人工智能/Pasted%20image%2020240422171948.png)


